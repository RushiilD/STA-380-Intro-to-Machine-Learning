---
title: "STA380 II"
author: "Rushiil Deshmukh"
date: "11/08/2021"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## R Markdown

STA380 II

# Question 1

```{r 1}
rm(list=ls())
library(glmnet)
library(ggplot2)
data1=read.csv('greenbuildings.csv')
summary(data1)
attach(data1)
```

## Part 1
Dropping 10% low occupancy
```{r 1_1_1}
ggplot(data1,aes(x = leasing_rate)) + geom_histogram(binwidth=11,color="black", fill="white") +
  labs(x='Leasing rate', y='Number', title = 'Leasing rate count')+ 
  geom_vline(aes(xintercept=median(leasing_rate)),
              color="red", linetype="dashed", size=1)+
  geom_vline(aes(xintercept=mean(leasing_rate)),
             color="blue", linetype="dashed", size=1)
```
Median leasing rate is ~90%, and most low occupancy reasons buildings are far 
enough away from the mean and median to be dropped without affecting our findings
significantly.

```{r 1_1_2}
lowoccup=data1[data1$leasing_rate == 0,]
lowoccup=na.omit(lowoccup)
ggplot(lowoccup, aes(renovated, ..count..)) + geom_bar()+
  labs(x='Renovated', y='Count', title = '0 occup,renovated count')

lowoccup2=data1[data1$leasing_rate < 0.1,]
lowoccup2=na.omit(lowoccup)
summary(lowoccup2)
```
There appear to be a notable number of buildings with 0 occupancy.
On further analysis, it is apparent that the median age for low occupancy 
buldings is 57 as opposed to 34 for the main dataset, as well as a lower number 
of renovated buildings as opposed to the main dataset.
This can be reason enough to neglect the buildings with an occupancy rate < 10%.


## Part 2
To test for a premium in green buildings, we cannot simply add the difference between the two medians as we are not accounting for different factors that also affect revenue. To test if there is a true difference in price per square-foot, ideally, we would run a regression and hold other variables constant to see the individual effect on rent for a green building versus a non-green building.

```{r 1_2}
plot(x = Electricity_Costs, y = Rent, main = "Rent vs Electricity Cost")
```
```{r 1_2_2}
Energystar <- as.factor(Energystar)
a <- aggregate(amenities ~ Energystar, data1, mean)
barplot(a$amenities, names.arg = a$Energystar, xlab = "EnergyStar", ylab = "Amenities", col = rainbow(2),main = 'Amenities for Energy star')
```

## Part 3
As seen above, as rent increases, electricity costs also increase, therefore one
cannot flat out assume a profit will stem with an increase in rent for green buildings.
Furthermore, energy efficient buildings are more likely to have amenities. This will increase overhead costs, further justifying a lack of certainty in a profit stemming from increased rents in green buildings.


## Part 4
The calculation for $5 million in extra construction costs for a green building is valid, and the Median occupancy rate is 90%, therefore it is safe to take a 90% test 
occupancy rate for recuperation calculations. This however does not mean the recuperation calculations are accurate due to variables not accounted for, as explained above.

## Part 5
```{r 1_5}
ggplot(data1,aes(x = factor(renovated), y = age)) + 
  geom_bar(stat='summary')
```
The stats guru assumes that the building will be earning rent for 30 years. This
assumption is accurate since the average age for non-renovated buildings is 35.
Therefore, the building will likely generate rent revenue for at least 30 years 
before undergoing a renovation. However, we cannot confirm that this is a 
financially good decision based purely on the stats guru's conclusions, due to 
the presence of multiple confounding variables that are unaccounted for, as depicted in part 4.


## Part 6
```{r 1_6_1}
ggplot(data = data1, aes(x = Energystar, y = class_a, fill = amenities)) + stat_summary(fun = sum, geom = "bar")
```
```{r 1_6_2}
ggplot(data = data1, aes(x = Energystar, y = class_b, fill = amenities)) + stat_summary(fun = sum, geom = "bar")
```

```{r 1_6_3}
ggplot(data = data1, aes(x = amenities, y = class_a, fill = amenities)) + stat_summary(fun = sum, geom = "bar")
```

```{r 1_6_4}
ggplot(data = data1, aes(x = amenities, y = class_b, fill = amenities)) + stat_summary(fun = sum, geom = "bar")
```
```{r 1_6_5}
ggplot(data1, aes(x = factor(green_rating), y = age)) + 
  geom_bar(stat = 'summary')
```

```{r 1_6_6}

```

As seen above, green status can also be impacted by other variables beyond rent. 
For example, building class type is correlated with green status, and with an increase
in class type, an increase in amenities is also seen. So, there is an increase in rent
from variables outside of green status, such as amenities. 
Some other confounding variables are Age and Employment Growth.

## Conclusion

For the majority of conclusions found by the stats guru, we are also unconvinced his findings are correct. The most common issue his work has is in not accounting for all variables that affect Rent. As we found confounding variables in our data set, we know that the variables play off one another for their true influence and in order to obtain accurate figures we need to include this in our work.


# Question 3 - Portfolio Modelling

```{r 3_1}
rm(list=ls())
library(mosaic)
library(quantmod)
library(foreach)
```

Here is a short description of the ETFs we chose for this model:

- Equity- Large Cap Growth - SPY
- Equity- Small Cap Growth - IWM
- Large Cap- Blend - RSP
- Bonds- UST 1-3 YR - SHY
- Hedge Fund - DBEF

## Part 1
Adjusting all stocks
```{r 3_2, message=FALSE, warning=FALSE}
mystocks = c("SPY", "IWM", "RSP", "SHY", "DBEF")
myprices = getSymbols(mystocks, from = "2015-12-01")
for(ticker in mystocks) {
  expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
  eval(parse(text=expr))
}
```


## Part 2 
Combine all the returns in a matrix
```{r 3_3}
all_returns = cbind( ClCl(SPYa),
                     ClCl(IWMa),
                     ClCl(RSPa),
                     ClCl(SHYa),
                     ClCl(DBEFa)
                     )
head(all_returns)
all_returns = as.matrix(na.omit(all_returns))
```


## Part 3 
Calculating returns for each ETF
```{r 3_4}
returns = data.frame(round(dailyReturn(SPYa)*100,4),
                     round(dailyReturn(IWMa)*100,4),round(dailyReturn(RSPa)*100,4),
                     round(dailyReturn(SHYa)*100,4),round(dailyReturn(DBEFa)*100,4))

colnames(returns)=mystocks
head(returns,10)
cor(returns)
```

Now, let's figure out how to set their weights accordingly. For this, we 
calculate the daily returns for each ETF anf find their the median and standard 
deviation to understand their mean returns and volatitlity.


## Part 4
Getting the median return and volatility for each ETF
```{r 3_5}
mystocks_Stats = do.call(data.frame, 
                    list(mean = round(apply(returns, 2, mean),4),
                         sd = round(apply(returns, 2, sd),4),
                         median = round(apply(returns, 2, median),4),
                         min = round(apply(returns, 2, min),4),
                         max = round(apply(returns, 2, max),4)))

t(mystocks_Stats)
```
As is visible from the above table, IWM has the highest volatility, followed by 
DBEF, RSP, SPY and finally SHY. This is congruent with our ETF behaviors so far.
The median value for DBEF is the highest, followed by IWM, RSP, SPY and SHP. 

## Part 5
Modelling the Portfolios

### Portfolio 1
Aggressive portfolio
```{r 3_6_1}
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.175, 0.3, 0.175, 0, 0.35)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}
```

Profit/loss
```{r 3_6_1_1}

mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
```

5% value at risk:
```{r 3_6_1_2}

abs(quantile(sim1[,n_days]- initial_wealth, prob=0.05))
```

Total Wealth Plot
```{r 3_6_1_3}
hist(sim1[,n_days], 25)
```

Net Profit Plot
```{r 3_6_1_4}
hist(sim1[,n_days]- initial_wealth, breaks=30)
```


### Portfolio 2
Balanced
``` {r 3_6_2}
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.2, 0.2, 0.2, 0.2, 0.2)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}

```

Profit/loss
```{r 3_6_2_1}
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
```

5% value at risk
```{r 3_6_2_2}
abs(quantile(sim1[,n_days]- initial_wealth, prob=0.05))
```

Total Wealth Plot
```{r 3_6_2_3}
hist(sim1[,n_days], 25)
```

Net Profit Plot
``` {r 3_6_2_4}
hist(sim1[,n_days]- initial_wealth, breaks=30)
```



### Portfolio 3
Safe
```{r 3_6_3}
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
  total_wealth = initial_wealth
  weights = c(0.1, 0.1, 0.1, 0.6, 0.1)
  holdings = weights * total_wealth
  n_days = 20
  wealthtracker = rep(0, n_days)
  for(today in 1:n_days) {
    return.today = resample(all_returns, 1, orig.ids=FALSE)
    holdings = holdings + holdings*return.today
    total_wealth = sum(holdings)
    wealthtracker[today] = total_wealth
  }
  wealthtracker
}
```

Profit/loss
``` {r 3_6_3_1}
mean(sim1[,n_days])
mean(sim1[,n_days] - initial_wealth)
```

5% value at risk
```{r 3_6_3_2}
abs(quantile(sim1[,n_days]- initial_wealth, prob=0.05))
```

Total Wealth Plot
```{r 3_6_3_3}
hist(sim1[,n_days], 25)
```

Net Profit Plot
```{r 3_6_3_4}
hist(sim1[,n_days]- initial_wealth, breaks=30)
```



## Conclusion

Aggressive Portfolio:  
In creating our aggressive portfolio, we avoided the UST Bond completely as we selected this knowing it has a lower expected return and risk. We did so to create a balance of risk, but as this is an aggressive approach, we aren’t concerned with that here. But as to how we would weigh our Equity ETFs and Hedge fund ETF, we tested the standard deviation and median return for all our ETFs and then allocated the weight accordingly based on those metrics. We found the Hedge fund to have the lowest risk while also having the highest expected return, so we valued ‘DBEF’ at 35%. The next most efficient stock was IWM and we placed a 30% weight there. ‘SPY’ and ‘RSP’ were about equal in terms of both risk and reward, so we gave each of those the remaining 17.5%.

Balanced Portfolio:  
We created a balanced portfolio and weighted each ETF equally at 20%. We found a profit of about $950 and a 5% VaR ≈ 6400. We did this in hope of getting baseline values and then being able to compare our safe and aggressive combinations.

Safe Portflio:  
We purposefully selected a UST Government Bond for our portfolio as those bonds are known to be “risk-free”, so when creating our Safe portfolio we assumed this would garner a large portion of the weight. To test this, we obtained the standard deviation and median return for all of our ETFs and then allocated the weight accordingly based on those metrics. Our portfolio is comprised of a combination of Small & Large Cap Growth Equities, a Value Equity fund, a Hedge Fund, and lastly the UST Government Bond. As we expected, the standard deviation and expected return was considerably less for the UST Bond than compared with the others so we placed 60% of our weight here. All the others had ranges similar to one another so in still wanting to be profitable we gave each Equity fund and the Hedge fund 10%. This portfolio combination had about $540 profit and our lowest 5% VaR of  ≈ 2900.



# Question 4

```{r 4_1}
rm(list=ls())
library(tidyverse)
library(glmnet)
library(ggplot2)
library(reshape2)
library(cluster)
library(mvtnorm)
library(factoextra)
library(NbClust)
library(foreach)
library(LICORS)
library(HSAUR)
library(fpc)
library(corrplot)

s_main=read.csv('social_marketing.csv')
```


## Part A

Removing unwanted columns and scaling the data
```{r 4_2}
s_1 = s_main[,2:36]
s_col = names(s_1) %in% c('','chatter','uncategorized','spam','adult')
s = s_main[,!s_col]
scaled_s=scale(s, center=TRUE, scale=TRUE)
```


## Part B

Plotting the correlation between variables
```{r 4_3}
mtrix_corr=round(cor(s), 2)
corrplot(mtrix_corr, method="circle")
```
As is visible from this correlation plot, a lot of variables in this dataset
are highly correlated. Certain examples of this are:  
- Politics and travel  
- Parenting and sports  
- Health nutrition and Personal fitness  

## Part C

Determining the optimum number of clusters

### Elbow method
```{r 4_4_1}
fviz_nbclust(scaled_s, kmeans, method = "wss")
```
We're unable to easily determine the right value of k from this plot alone. 
However, the line seems to stabilize after k=7.

### Silhouette method
```{r 4_4_2}
fviz_nbclust(scaled_s, kmeans, method = "silhouette")+labs(subtitle = "Silhouette method")
```
This plot shows optimum number of clusters at 2, but intuitively it seems too 
less. The silhouette width tends to stabilize at k=5, before further reducing at
k=8.

### CH method
```{r 4_4_3, warning=FALSE}
k_grid = seq(2,20,by=1)
N = nrow(scaled_s) 
CH_grid = foreach(k = k_grid, .combine='c') %do% { 
  cluster_k = kmeans(scaled_s, k, nstart=50) 
  W = cluster_k$tot.withinss 
  B = cluster_k$betweenss 
  CH = (B/W)*((N-k)/(k-1)) 
  CH 
} 
plot(k_grid, CH_grid)
```
The first dip appears to be at k=2, thereby again giving us 2 optimal clusters.


## Part D

Principal Component Analysis  
We run PCA to reduce the number of correlated variables. This will allow us to
segment the data into the appropriate categories later on.  

Generating the PCA
```{r 4_5_1}
scaled_pca = prcomp(s, scale=TRUE, center = TRUE)
summary(scaled_pca)
plot(scaled_pca, type= 'l')
```

Calculating cumulative variance
```{r 4_5_2}
variance_pca=scaled_pca$sdev ^ 2
variance_pca_a=variance_pca/sum(variance_pca)
#Cumulative sum of variation explained
plot(cumsum(variance_pca_a), xlab = "Principal Component", 
     ylab = "Fraction of variance explained")
cumsum(variance_pca_a)[10]
```
By the Kaiser criterion, we should drop all principal components with
eigen values < 1.

Generating PCA data
```{r 4_5_3}
varimax(scaled_pca$rotation[, 1:11])$loadings
scores = scaled_pca$x
pc_data=as.data.frame(scores[,1:18])
S=pc_data
```


## Part E

K means clustering  

After running PCA and identifying various plots, we chose to proceed with k-means
clustering method using 4 clusters and 25 starts. This is because 4 segments were
found to be the most practical in terms of data interpret ability and prevented
any significant overlap of cluster data.

Running K means
```{r 4_6_1}
fviz_nbclust(S, kmeans, method = "wss")

clust_k = kmeanspp(S, 4, nstart=25)

social_clust_k=cbind(S, clust_k$cluster)
plotcluster(S, clust_k$cluster)
```

Re visualizing the same plot
```{r 4_6_2}
kmeans_2 = kmeans(S,4,nstart=25)
fviz_cluster(kmeans_2,data=S,geom=c('point'),ellipse.type='euclid')
```

### Plotting the clusters

Calculatig mu and sigma
```{r 4_7_1,warning=FALSE}
mu = attr(scaled_s,"scaled:center")
sigma = attr(scaled_s,"scaled:scale")

social_clust_k_1 = as.data.frame(cbind(clust_k$center[1,]*sigma + mu, 
                                       clust_k$center[2,]*sigma + mu,
                                       clust_k$center[3,]*sigma + mu,
                                       clust_k$center[4,]*sigma + mu))

names(social_clust_k_1)=c('Cluster_1','Cluster_2','Cluster_3','Cluster_4')
#,'Cluster_5')
social_clust_k_1$type=row.names(social_clust_k_1)
summary(social_clust_k_1)
```


Cluster 1
````{r 4_7_2}
ggplot(social_clust_k_1, aes(x =reorder(type, -Cluster_1) , y=Cluster_1)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 1",
       x ="Category", y = "Cluster centre values") 
```

Cluster 2
```{r 4_7_3}
ggplot(social_clust_k_1, aes(x =reorder(type, -Cluster_2) , y=Cluster_2)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 2",
       x ="Category", y = "Cluster centre values")
```

Cluster 3
```{r 4_7_4}
ggplot(social_clust_k_1, aes(x =reorder(type, -Cluster_3) , y=Cluster_3)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 3",
       x ="Category", y = "Cluster centre values")
```

Cluster 4
```{r 4_7_5}
ggplot(social_clust_k_1, aes(x =reorder(type, -Cluster_4) , y=Cluster_4)) +
  geom_bar(stat="identity", position ="dodge") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle=-40, hjust=.1)) + 
  labs(title="Cluster 4",
       x ="Category", y = "Cluster centre values")
```


## Result
Based on this analysis, we have identified the following market segments:  
- 1. Current Events, Travel, Computers  
- 2. Health/Nutrition, Cooking, Personal Fitness   
- 3. Sports Fandom, Politics  
- 4. College_uni,news,online_gaming  

Following are certain inferences based on each segment:  
- 1. The Informed - Loves to stay ahead of things and well read about world events
                    Middle aged
- 2. The fitness enthusiasts - Pretty self evident, like to watch their food intake
                               and love breaking a sweat. Young.
- 3. The Average Joe - Stays out of most discussions, but can go on for a long time
                       about their favorite team on and off the field. Middle aged
                       to old.
- 4. The Student - College going students who like to unwind online after a day's
                   worth of hard work. Young.